{"componentChunkName":"component---src-templates-va-gov-team-template-jsx","path":"/platform/working-with-vsp/policies-work-norms/how-to-write-your-slos","result":{"pageContext":{"html":"<hr>\n<h1>We've moved our docs!</h1>\n<h3>This document is no longer maintained.</h3>\n<h3>Please visit the <a href=\"https://depo-platform-documentation.scrollhelp.site/\">Platform website</a> for the latest information or contact the Platform Support Team via <a href=\"https://dsva.slack.com/archives/CBU0KDSB1\">#vfs-platform-support</a>.</h3>\n<hr>\n<h2>How to write your SLOs</h2>\n<p>Status: <strong>DRAFT</strong></p>\n<p>Last updated: <strong>2019 May 13</strong></p>\n<h2>SLO Guidance</h2>\n<h3>SLOs vs SLAs</h3>\n<p><strong>SLOs</strong>: service level <strong>objectives</strong>. These are targets that a service team wants its system to achieve, and will measure and report its performance against those targets. However, there are no consequences if the objective is not met --- these are aspirational but realistic goals that the service team is communicating <em>should be</em> the performance of the measured service.</p>\n<p><strong>SLAs</strong>: service level <strong>agreements</strong>. These are targets that a service team intends its system to hit, with <strong>consequences</strong> if those targets are not hit.</p>\n<h3>What kinds of measurements should an SLO include?</h3>\n<p>SLOs should measure <em>user-experienced performance</em>:</p>\n<ul>\n<li>\n<p><strong>User-experienced</strong>: this refers to the user of the system and what they perceive.</p>\n<p>For a backend, the user is typically another service making programmatic calls. The perceived performance of the system is typically measured in availability, latency, and error rates.</p>\n<p>For a website/frontend/user application, the metrics might be:</p>\n<ul>\n<li>Page load time (latency)</li>\n<li>Page load success rate (availability / error rate)</li>\n<li>Success rate of user actions, such as logging in, or submitting claims in an application.</li>\n</ul>\n</li>\n<li><strong>Performance</strong>: this refers primarily to technical performance metrics, such as availability/error rates, and latency. This <em>excludes</em> categories such as product success metrics that indicate whether a service is understandable to users, easy to navigate or whether a product or feature meets a service objective of the VA.</li>\n</ul>\n<h3>How should these metrics be measured?</h3>\n<h4>Availability / error rates</h4>\n<p>Availability, at its simplest, should be a measurement of:</p>\n<pre><code>|all requests| - |failed requests|\n----------------------------------\n         |all requests|\n</code></pre>\n<p>This ratio is typically reported every pre-determined period of time (such as one day, or one quarter) that can describe the performance of the system with a large enough granularity to see trends over time. <em>Is the system improving week-to-week as reliability improvements are made? Has our system’s availability worsened this quarter compared to last quarter?</em></p>\n<p><em>Note: this is explicitly <strong>not</strong> time-based. If a service is entirely unavailable for an hour of peak traffic, that will hurt the availability metric much more than an hour unavailable during off-hours. This reflects the fact that SLO measures \\</em>user-experienced performance<em>.\\</em></p>\n<ul>\n<li>\n<p><strong>all requests</strong>: all requests made to this service in the given time period, as perceived by the user.</p>\n<p>For a backend, this typically corresponds 1:1 with API calls.</p>\n<p>For a frontend/website, this typically corresponds to <em>page loads</em> or <em>user actions</em>, as opposed to <em>HTTP requests</em>, since a single user-perceived event (i.e. <em>loading VA.gov</em> or <em>logging in</em>) consists of many HTTP requests.</p>\n</li>\n<li>\n<p><strong>failed requests</strong>: the subset of all requests that failed to be correctly served.</p>\n<p>For a backend, this typically means requests that either:</p>\n<ul>\n<li>return a response code indicating service failure (i.e. an HTTP 5xx response class), or</li>\n<li>return an error in the response data</li>\n</ul>\n<p>For a frontend, this typically means that the user action failed in ways caused by the* service*, not the <em>user</em>. For example, when a user fails a login attempt, if that failure is due to:</p>\n<ul>\n<li>user error (i.e. an invalid password), that is <em>not</em> a failed request</li>\n<li>anything else, that is a <strong>failed request</strong></li>\n</ul>\n<p><em>Note: for some services, it might be useful for the SLO template to <strong>explicitly</strong> define which response codes are considered \"failed\".</em></p>\n</li>\n</ul>\n<h4>Latency</h4>\n<p>Latencies are typically considered as a distribution, often with a long tail tail that spikes upwards in latency. For the purposes of user-perceived reliability, latency should be measured in an SLO against a few chosen percentiles, including at least one percentile to cover \"typical cases\" and another percentile to cover</p>\n<p>For example, if an SLO that declared its latency as:</p>\n<p>\"90% @ 200ms, 99% @ 1500ms\"</p>\n<p>This would indicate an objective that 90% of all requests to this service are responded to in under 200ms, and 99% of all requests are responded to in under 1500ms.</p>\n<p>For backend services, latency should be measured as end-to-end as is visible from the backend service itself. That is, latency should be measured as the time from when the incoming request was first received to the time when that request has been responded to.</p>\n<p>For user actions served by frontend services (such as a login event or form submission), measurement can get closer to the <em>user-perceived latency</em> by measuring the event’s latency from the client browser rather than from the backend service. This adds some variability due to network latency and client network situations, but provides a more realistic view of latency as experienced by users.</p>\n<p>For webpage loads, <em>user-perceived latency</em> is more complicated to measure. This can typically be measured a number of ways:</p>\n<ul>\n<li>Time to First Byte (TTFB), which indicates that the HTTP response has been built and is starting to be streamed to the user. This measures server latency more than end-to-end user experience, and can be measured easily from external probers sending requests to the website.</li>\n<li><a href=\"https://developers.google.com/web/tools/lighthouse/audits/first-contentful-paint\">First Contentful Paint (FCP)</a>. This measures the time to the first visual content rendered in the browser, which provides the user feedback that the page is successfully loading. This can be measured more easily from client-side using performance tools or analytics libraries.</li>\n<li><a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Document/DOMContentLoaded_event\"><code>DOMContentLoaded</code></a>, an event triggered when a webpage has fully loaded and is completely ready to interact with. This can be measured more easily from client-side using performance tools or analytics libraries.</li>\n</ul>\n<h4>Reporting</h4>\n<p>Availability and latency metrics should be aggregated over periods of time that are large enough to see trends useful for strategic decision-making about reliability.</p>\n<p>Typically, these time periods should be <strong>daily</strong>, <strong>monthly</strong>, and/or <strong>quarterly</strong>.</p>\n<p>Reporting SLOs over these time periods may require changes as to how SLO data is recorded, depending on the system used for aggregating data.</p>\n<h2>SLO Template</h2>\n<p><strong>Instructions</strong>: Copy this table for use as an SLO, adding rows as needed for each backend service method, page load, or user action that should be measured.</p>\n<p>Each row should have an availability target, and one or more latency targets. Include notes describing how availability and latency are measured, particularly for any potential ambiguity.</p>\n<table>\n  <tr>\n   <td>\n   </td>\n   <td>Availability \n   </td>\n   <td>Latency \n   </td>\n  </tr>\n  <tr>\n   <td>[Request being measured: backend method(s) for backend services, page load, user action for frontend services]\n   </td>\n   <td>[target success rate]\n   </td>\n   <td>\n<ul>\n<li>[percentile] @ [target latency]\n<p>[If relevant, notes on how this latency is measured]</p>\n</li>\n</ul>\n   </td>\n  </tr>\n  <tr>\n   <td>\n   </td>\n   <td>\n   </td>\n   <td>\n<ul>\n<li>[percentile] @ [target latency]\n</li>\n</ul>\n   </td>\n  </tr>\n</table>\n<h3>Example: backend service</h3>\n<table>\n  <tr>\n   <td><strong><code>WidgetsService</code></strong>\n   </td>\n   <td>Availability \n   </td>\n   <td>Latency \n   </td>\n  </tr>\n  <tr>\n   <td>\n      <code>CreateWidget|UpdateWidget|DeleteWidget</code>\n   </td>\n   <td>99.95%</td>\n   <td>\n<ul>\n<li>90% @ 300ms|99% @ 1000ms</li>\n</ul>\n   </td>\n  </tr>\n  <tr>\n   <td><code>GetWidget</code>\n   </td>\n   <td>99.99%\n   </td>\n   <td>\n<ul>\n<li>90% @ 50ms|99% @ 250ms</li>\n</ul>\n   </td>\n  </tr>\n  <tr>\n   <td><code>ListWidgets</code>\n   </td>\n   <td>99.99%\n   </td>\n   <td>\n<ul>\n<li>90% @ 200ms|99% @ 2000ms</li>\n</ul>\n   </td>\n  </tr>\n</table>\n<h3>Example: GI Bill Comparison Tool</h3>\n<table>\n  <tr>\n   <td>\n   </td>\n   <td>Availability \n   </td>\n   <td>Latency \n   </td>\n  </tr>\n  <tr>\n   <td>GI Bill Comparison page load\n   </td>\n   <td>99.9%\n   </td>\n   <td>\n<ul>\n<li>90% @ 300ms|99% @ 1000ms (measuring DOMContentLoaded)</li>\n</ul>\n   </td>\n  </tr>\n  <tr>\n   <td>School search results returned\n   </td>\n   <td>99.9%\n   </td>\n   <td>\n<ul>\n<li>90% @ 50ms|99% @ 300ms</li>\n</ul>\n   </td>\n  </tr>\n</table>","source":"va.gov-team","sourceUrl":"https://github.com/department-of-veterans-affairs/va.gov-team/blob/master/platform/working-with-vsp/policies-work-norms/how-to-write-your-slos.md","title":"We've moved our docs!"}},"staticQueryHashes":["2744294623","3649515864"]}